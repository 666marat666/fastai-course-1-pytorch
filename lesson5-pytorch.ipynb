{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodrigo/Libs/torchsample/torchsample/datasets.py:16: UserWarning: Cant import nibabel.. Cant load brain images\n",
      "  warnings.warn('Cant import nibabel.. Cant load brain images')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from torchsample.initializers import Uniform\n",
    "from torchsample.modules import ModuleTrainer\n",
    "from torchsample.metrics import CategoricalAccuracy\n",
    "\n",
    "%aimport torchsample.modules\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the reviews using code copied from keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import pickle\n",
    "\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 1st review. As you see, the words have been replaced by ids. The ids can be looked up in idx2word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word of the first review is 23022. Let's see what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[23022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the whole review, mapped from ids to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are 1 for positive, 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_tensor = torch.from_numpy(np.array(labels_train))\n",
    "labels_test_tensor = torch.from_numpy(np.array(labels_test))\n",
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i < vocab_size - 1 else vocab_size - 1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i < vocab_size - 1 else vocab_size - 1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of lengths of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, trn)))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)\n",
    "\n",
    "trn_tensor = torch.from_numpy(trn).long()\n",
    "test_tensor = torch.from_numpy(test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in nice rectangular matrices that can be passed to ML algorithms. Reviews shorter than 500 words are pre-padded with zeros, those greater are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000, 500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleHiddenLayerModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_dimensions = 32\n",
    "        self.embedding = nn.Embedding(vocab_size, num_dimensions)\n",
    "        self.fc1 = nn.Linear(seq_len * num_dimensions, 100)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, words_ids):\n",
    "        x = self.embedding(words_ids) # x => torch.Size([64, 500, 32])\n",
    "        x = x.view(x.size(0), -1) # x => torch.Size([64, 16000])\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # result = F.sigmoid(x)\n",
    "        result = x\n",
    "        return result\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.constant(self.fc1.bias, val=0.0)\n",
    "        torch.nn.init.constant(self.fc2.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleHiddenLayerModule (\n",
       "  (embedding): Embedding(5000, 32)\n",
       "  (fc1): Linear (16000 -> 100)\n",
       "  (dropout): Dropout (p = 0.7)\n",
       "  (fc2): Linear (100 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = SingleHiddenLayerModule()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "trainer.set_initializers([Uniform(module_filter=\"embedding*\", a=-0.05, b=0.05), XavierUniform(module_filter=\"fc*\")])\n",
    "trainer.set_metrics([CategoricalAccuracy()])\n",
    "\n",
    "# trainer.summary((trn_tensor.size(0), labels_train_tensor.size(0)))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 392 batches [01:19,  2.72s/ batches, val_acc=81.47, val_loss=0.2871, acc=75.19, loss=0.4612]\n",
      "Epoch 2/2: 392 batches [01:40,  6.92s/ batches, val_acc=89.91, val_loss=0.3095, acc=92.56, loss=0.1991]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=2, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [stanford paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) that this dataset is from cites a state of the art accuracy (without unlabelled data) of 0.883. ~~So we're short of that, but on the right track.~~ We've already beaten the state of the art in 2011 with a simple Neural Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CnnMaxPoolingModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_dimensions = 32\n",
    "        self.embedding = nn.Embedding(vocab_size, num_dimensions)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.conv1 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2, groups=1)\n",
    "        self.fc1 = nn.Linear(seq_len * num_dimensions, 100)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, words_ids):\n",
    "        x = self.embedding(words_ids)  # x => torch.Size([B, 500, 32])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # print('emb', x.size())\n",
    "        x = self.drop1(x) # x => torch.Size([B, 500, 32])\n",
    "        x = self.conv1(x)   # x => torch.Size([B, 500, 64])\n",
    "        x = F.relu(x, True)\n",
    "        # print('conv1', x.size())\n",
    "        x = self.drop1(x)   # x => torch.Size([B, 500, 64])\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        # print('max', x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # result = F.sigmoid(x)\n",
    "        result = x\n",
    "        \n",
    "        #raise 'Error'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.constant(self.conv1.bias, val=0.0)\n",
    "        torch.nn.init.constant(self.fc1.bias, val=0.0)\n",
    "        torch.nn.init.constant(self.fc2.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CnnMaxPoolingModule (\n",
       "  (embedding): Embedding(5000, 32)\n",
       "  (drop1): Dropout (p = 0.2)\n",
       "  (conv1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,), groups=2)\n",
       "  (fc1): Linear (16000 -> 100)\n",
       "  (dropout): Dropout (p = 0.7)\n",
       "  (fc2): Linear (100 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CnnMaxPoolingModule()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "trainer.set_initializers([Uniform(module_filter=\"embedding*\", a=-0.05, b=0.05), XavierUniform(module_filter=\"fc*\"), XavierUniform(module_filter=\"conv*\")])\n",
    "trainer.set_metrics([CategoricalAccuracy()])\n",
    "\n",
    "# trainer.summary((trn_tensor.size(0), labels_train_tensor.size(0)))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 392 batches [04:24, 18.87s/ batches, val_acc=80.01, val_loss=0.2810, acc=71.68, loss=0.5062]\n",
      "Epoch 2/2: 392 batches [03:50, 17.43s/ batches, val_acc=89.87, val_loss=0.2576, acc=90.39, loss=0.2602]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=2, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 392 batches [02:59,  7.73s/ batches, val_acc=77.54, val_loss=0.2922, acc=67.17, loss=0.5545]\n",
      "Epoch 2/4: 392 batches [02:03,  7.54s/ batches, val_acc=89.12, val_loss=0.2622, acc=89.18, loss=0.2754]\n",
      "Epoch 3/4: 392 batches [02:30,  9.34s/ batches, val_acc=90.64, val_loss=0.2588, acc=92.10, loss=0.2103]\n",
      "Epoch 4/4: 392 batches [02:40,  9.55s/ batches, val_acc=91.03, val_loss=0.2911, acc=93.66, loss=0.1727]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to look at wordvectors.ipynb before moving on.\n",
    "\n",
    "In this section, we replicate the previous CNN, but using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading word vectors from ./glove.6B.50d.txt: 100%|██████████| 400000/400000 [00:10<00:00, 38319.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from torchtext.vocab import load_word_vectors\n",
    "\n",
    "wv_dict, wv_arr, wv_size = load_word_vectors('.', 'glove.6B', 50)\n",
    "\n",
    "print('Loaded', len(wv_arr), 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word(word):\n",
    "    return wv_arr[wv_dict[word]]\n",
    "\n",
    "def create_emb():\n",
    "    num_dimensions_glove = wv_arr.size()[1]\n",
    "    \n",
    "    embedding = nn.Embedding(vocab_size, num_dimensions_glove)\n",
    "    # If we can't find the word in glove, randomly initialize\n",
    "    torch.nn.init.uniform(embedding.weight, a=-0.05, b=0.05)\n",
    "\n",
    "    num_found, num_not_found = 0, 0\n",
    "    \n",
    "    for i in range(1,len(embedding.weight)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            embedding.weight.data[i] = get_word(word)\n",
    "            num_found += 1\n",
    "        else:\n",
    "            num_not_found +=1\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    torch.nn.init.uniform(embedding.weight.data[-1], a=-0.05, b=0.05)\n",
    "    embedding.weight.requires_grad = False\n",
    "    \n",
    "    # This speeds up training. Can it be replaced by BatchNorm1d?\n",
    "    embedding.weight.data /= 3\n",
    "    \n",
    "    print(\"Words found: {}, not found: {}\".format(num_found, num_not_found))\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CnnMaxPoolingModuleWithEmbedding(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super().__init__()\n",
    "        num_dimensions = 32\n",
    "        self.embedding = embedding\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.batchnorm = nn.BatchNorm1d(500)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding.weight.size()[1], out_channels=64, kernel_size=5, padding=2, groups=1)\n",
    "        self.fc1 = nn.Linear(seq_len * num_dimensions, 100)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, words_ids):\n",
    "        x = self.embedding(words_ids)\n",
    "        # x = self.batchnorm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x, True)\n",
    "        x = self.drop1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        result = x\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.constant(self.conv1.bias, val=0.0)\n",
    "        torch.nn.init.constant(self.fc1.bias, val=0.0)\n",
    "        torch.nn.init.constant(self.fc2.bias, val=0.0)\n",
    "        \n",
    "    def parameters(self):\n",
    "        p = filter(lambda p: p.requires_grad, nn.Module.parameters(self))\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found: 4914, not found: 85\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "emb = create_emb()\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CnnMaxPoolingModuleWithEmbedding(emb)\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "trainer.set_initializers([XavierUniform(module_filter=\"fc*\"), XavierUniform(module_filter=\"conv*\")])\n",
    "trainer.set_metrics([CategoricalAccuracy()])\n",
    "\n",
    "# trainer.summary((trn_tensor.size(0), labels_train_tensor.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 392 batches [05:23, 28.35s/ batches, loss=0.6532, acc=61.48, val_loss=0.5240, val_acc=69.12]\n",
      "Epoch 2/10: 392 batches [04:36, 18.35s/ batches, loss=0.5188, acc=75.36, val_loss=0.4707, val_acc=77.29]\n",
      "Epoch 3/10: 392 batches [04:55, 18.07s/ batches, loss=0.4685, acc=78.90, val_loss=0.4274, val_acc=80.36]\n",
      "Epoch 4/10: 392 batches [03:10, 14.13s/ batches, loss=0.4317, acc=80.46, val_loss=0.3984, val_acc=81.39]\n",
      "Epoch 5/10: 392 batches [02:58, 13.89s/ batches, loss=0.4150, acc=81.47, val_loss=0.3811, val_acc=82.45]\n",
      "Epoch 6/10: 392 batches [03:30, 15.72s/ batches, loss=0.3967, acc=82.54, val_loss=0.3822, val_acc=83.00]\n",
      "Epoch 7/10: 392 batches [03:31, 14.43s/ batches, loss=0.3758, acc=83.47, val_loss=0.3835, val_acc=83.19]\n",
      "Epoch 8/10: 392 batches [03:06, 14.12s/ batches, loss=0.3543, acc=84.56, val_loss=0.3692, val_acc=84.13]\n",
      "Epoch 9/10: 392 batches [02:55, 10.51s/ batches, loss=0.3453, acc=84.74, val_loss=0.3665, val_acc=84.38]\n",
      "Epoch 10/10: 392 batches [03:03, 12.91s/ batches, loss=0.3295, acc=85.66, val_loss=0.3823, val_acc=84.21]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=10, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.embedding.weight.requires_grad = True\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-4)\n",
    "trainer.set_loss(criterion)\n",
    "trainer.set_metrics([CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 392 batches [01:50,  6.58s/ batches, loss=0.3015, acc=87.19, val_loss=0.3479, val_acc=86.01]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-size CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of a multi-size CNN as shown in Ben Bowles' excellent blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create multiple conv layers of different sizes, and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CnnMaxPoolingModuleMultiSizeWithEmbedding(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super().__init__()\n",
    "        num_dimensions = 32\n",
    "        self.embedding = embedding\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.batchnorm = nn.BatchNorm1d(500)\n",
    "        self.convs = [self.create_conv(embedding, fsz) for fsz in range (3, 6)]\n",
    "        self.fc1 = nn.Linear(25000, 100)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        self.init()\n",
    "        \n",
    "    def create_conv(self, embedding, fsz):\n",
    "        return nn.Conv1d(in_channels=embedding.weight.size()[1], out_channels=64, kernel_size=5, padding=2, groups=1)\n",
    "    \n",
    "    def conv(self, c, x):\n",
    "        x = c(x)\n",
    "        x = F.relu(x, True)\n",
    "        x = self.drop1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, words_ids):\n",
    "        x = self.embedding(words_ids)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.drop1(x)\n",
    "        convs = [self.conv(conv, x) for conv in self.convs]\n",
    "        \n",
    "        torch.cat(convs, dim=1)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        result = x\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.constant(self.fc1.bias, val=0.0)\n",
    "        torch.nn.init.constant(self.fc2.bias, val=0.0)\n",
    "        for conv in self.convs:\n",
    "            torch.nn.init.xavier_uniform(conv.weight.data, gain=1.0)\n",
    "            torch.nn.init.constant(conv.bias, val=0.0)\n",
    "        \n",
    "    def parameters(self):\n",
    "        p = filter(lambda p: p.requires_grad, nn.Module.parameters(self))\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found: 4914, not found: 85\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "emb = create_emb()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CnnMaxPoolingModuleMultiSizeWithEmbedding(emb)\n",
    "model.embedding.weight.requires_grad = True\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "trainer.set_initializers([XavierUniform(module_filter=\"fc*\")])\n",
    "trainer.set_metrics([CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 392 batches [10:42, 73.36s/ batches, loss=0.6308, acc=61.92, val_loss=0.4106, val_acc=72.10]\n",
      "Epoch 2/10: 392 batches [13:00, 70.60s/ batches, loss=0.4033, acc=81.30, val_loss=0.3117, val_acc=84.12]\n",
      "Epoch 3/10: 392 batches [08:08, 28.02s/ batches, loss=0.2956, acc=87.68, val_loss=0.2967, val_acc=87.63]\n",
      "Epoch 4/10: 392 batches [05:14, 33.21s/ batches, loss=0.2316, acc=90.32, val_loss=0.3037, val_acc=88.94]\n",
      "Epoch 5/10: 392 batches [05:00, 23.83s/ batches, loss=0.1827, acc=92.86, val_loss=0.3306, val_acc=90.25]\n",
      "Epoch 6/10: 392 batches [04:18, 25.20s/ batches, loss=0.1464, acc=94.25, val_loss=0.3538, val_acc=90.86]\n",
      "Epoch 7/10: 392 batches [04:17, 21.28s/ batches, loss=0.1203, acc=95.12, val_loss=0.3973, val_acc=91.11]\n",
      "Epoch 8/10: 392 batches [04:28, 27.56s/ batches, loss=0.0926, acc=96.44, val_loss=0.4414, val_acc=91.88]\n",
      "Epoch 9/10: 392 batches [04:46, 29.97s/ batches, loss=0.0766, acc=97.03, val_loss=0.4905, val_acc=92.07]\n",
      "Epoch 10/10: 392 batches [05:13, 32.26s/ batches, loss=0.0649, acc=97.38, val_loss=0.5613, val_acc=92.25]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=10, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly over-fitting. But it does get the highest accuracy on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't covered this bit yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LstmEmbeddingModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_dimensions = 32\n",
    "        self.num_hidden = 100\n",
    "        self.embedding = nn.Embedding(vocab_size, num_dimensions)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=self.num_hidden, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(50000, 2)\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, words_ids):\n",
    "        \n",
    "        # We detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        # self.hidden = self.repackage_hidden(self.hidden)\n",
    "        \n",
    "        x = self.embedding(words_ids)\n",
    "        x = self.drop1(x)\n",
    "        #print('embd', x.size())\n",
    "        \n",
    "        self.hidden = self.init_hidden(x.size(0))\n",
    "        \n",
    "        #lenghts = [vocab_size for _ in range(x.size(0))]\n",
    "        #x = torch.nn.utils.rnn.pack_padded_sequence(x, lenghts, batch_first=True)\n",
    "        \n",
    "        #print('pack', x.data.size())\n",
    "        \n",
    "        x, self.hidden = self.lstm1(x, self.hidden)\n",
    "        \n",
    "        #print('lstm', x.data.size())\n",
    "        \n",
    "        #x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        \n",
    "        #print('unpk', x.size())\n",
    "        \n",
    "        # print(self.hidden)\n",
    "        # TODO can we get rid of contiguous?\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        #print('view', x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, True)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.constant(self.fc1.bias, val=0.0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        num_layers = 1\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(num_layers, batch_size, self.num_hidden).zero_()),\n",
    "                    Variable(weight.new(num_layers, batch_size, self.num_hidden).zero_()))\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == Variable:\n",
    "            return Variable(h.data)\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LstmEmbeddingModule()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "# TODO init LSTM\n",
    "trainer.set_initializers([Uniform(module_filter=\"embedding*\", a=-0.05, b=0.05), XavierUniform(module_filter=\"fc*\"), XavierUniform(module_filter=\"conv*\")])\n",
    "trainer.set_metrics([CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 392 batches [40:42, 66.02s/ batches, loss=0.6943, acc=50.00, val_loss=0.6931, val_acc=50.00]\n",
      "Epoch 2/5:   1%|▏         | 5/391 [00:10<12:52,  2.00s/ batches, loss=0.6931, acc=51.95]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-40bd787d104f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n\u001b[0;32m----> 2\u001b[0;31m             nb_epoch=5, batch_size=batch_size, shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/Users/rodrigo/Libs/torchsample/torchsample/modules/module_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, targets, validation_data, nb_epoch, batch_size, shuffle, cuda_device, verbose)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0;31m# backward pass and optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigo/Libs/pytorch/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO figure out how to do this in PyTorch\n",
    "trainer.fit(trn_tensor, labels_train_tensor, validation_data=(test_tensor, labels_test_tensor), \n",
    "            nb_epoch=5, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
