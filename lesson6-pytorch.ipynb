{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodrigo/Libs/torchsample/torchsample/datasets.py:16: UserWarning: Cant import nibabel.. Cant load brain images\n",
      "  warnings.warn('Cant import nibabel.. Cant load brain images')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from torchsample.initializers import Uniform\n",
    "from torchsample.modules import ModuleTrainer\n",
    "from torchsample.metrics import CategoricalAccuracy\n",
    "\n",
    "%aimport torchsample.modules\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to download the collected works of Nietzsche to use as our data for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0, \"\\0\")\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have a zero value in the dataset, e.g. for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÆäæéë'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map from chars to indices and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx will be the data we use from now own - it simply converts all the characters to their index (based on the mapping above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200297,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)\n",
    "x3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200297,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.stack(c4_dat)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 4 inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([40, 30, 29,  1]), array([42, 25,  1, 43]), array([29, 27,  1, 45]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:4], x2[:4], x3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29,  1, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200297,), (200297,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of latent factors to create (i.e. the size of the embedding matrix). Pick a size for our hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fac = 42\n",
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seq_len = 3\n",
    "\n",
    "def tensor(from_int):\n",
    "    return torch.from_numpy(np.array(from_int)).long()\n",
    "\n",
    "class SimpleRnn3Chars(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_fac)\n",
    "        self.dense_in_lin = nn.Linear(n_fac, n_hidden)\n",
    "        self.dense_hidden_lin = nn.Linear(n_hidden, n_hidden)\n",
    "        self.dense_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init()\n",
    "        # print(self.embedding(Variable(tensor([10]))))\n",
    "        # print(self.dense_in_lin.bias)\n",
    "        \n",
    "    def dense_in(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense_in_lin(x)\n",
    "        x = F.relu(x, True)\n",
    "        return x\n",
    "            \n",
    "    def dense_hidden(self, x):\n",
    "        x = self.dense_hidden_lin(x)\n",
    "        x = F.tanh(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, c1, c2, c3):\n",
    "        c1_in = self.embedding(c1) # x => torch.Size([B, 3, n_fac])\n",
    "        c2_in = self.embedding(c2)\n",
    "        c3_in = self.embedding(c3)\n",
    "        \n",
    "        c1_hidden = self.dense_in(c1_in)\n",
    "        \n",
    "        c2_dense = self.dense_in(c2_in)\n",
    "        hidden_2 = self.dense_hidden(c1_hidden)\n",
    "        c2_hidden = c2_dense + hidden_2\n",
    "        \n",
    "        c3_dense = self.dense_in(c3_in)\n",
    "        hidden_3 = self.dense_hidden(c2_hidden)\n",
    "        c3_hidden = c3_dense + hidden_3\n",
    "        \n",
    "        c4_out = self.dense_out(c3_hidden)\n",
    "        \n",
    "        return c4_out\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.uniform(self.embedding.weight, a=-0.05, b=0.05)\n",
    "        torch.nn.init.xavier_uniform(self.dense_in_lin.weight)\n",
    "        torch.nn.init.constant(self.dense_in_lin.bias, val=0.0)\n",
    "        torch.nn.init.eye(self.dense_hidden_lin.weight)\n",
    "        torch.nn.init.constant(self.dense_hidden_lin.bias, val=0.0)\n",
    "        torch.nn.init.xavier_uniform(self.dense_out.weight)\n",
    "        torch.nn.init.constant(self.dense_out.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRnn3Chars (\n",
       "  (embedding): Embedding(85, 42)\n",
       "  (dense_in_lin): Linear (42 -> 256)\n",
       "  (dense_hidden_lin): Linear (256 -> 256)\n",
       "  (dense_out): Linear (256 -> 85)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = SimpleRnn3Chars()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 3131 batches [00:21, 142.92 batches/s, loss=2.4394]                    \n",
      "Epoch 2/4: 3131 batches [00:20, 151.08 batches/s, loss=2.2161]                    \n",
      "Epoch 3/4: 3131 batches [00:20, 150.22 batches/s, loss=2.1526]                    \n",
      "Epoch 4/4: 3131 batches [00:20, 149.20 batches/s, loss=2.1232]                    \n"
     ]
    }
   ],
   "source": [
    "trainer.fit([tensor(x1), tensor(x2), tensor(x3)], tensor(y), nb_epoch=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [tensor([i]) for i in idxs]\n",
    "    p = trainer.predict(arrs)\n",
    "    # torch doesn't have an argmax function. See https://discuss.pytorch.org/t/argmax-with-pytorch/1528\n",
    "    v, i = torch.max(p, 1) # i is the result Tensor with the index locations of the maximum values\n",
    "    i = torch.max(i.data) # find any index (they are all max)\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' an')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the size of our unrolled RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 75111)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs, cs)]\n",
    "            for n in range(cs)]\n",
    "len(c_in_dat), len(c_in_dat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a list of the next character in each of these series. This will be the labels for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (75111,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_out_dat = [idx[i+cs] for i in range(0, len(idx)-1-cs, cs)]\n",
    "xs = [np.stack(c) for c in c_in_dat]\n",
    "len(xs), xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each column below is one series of 8 characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 33,  2, 72, 67, 73,  2]),\n",
       " array([42,  1, 38, 44,  2,  9, 61, 73]),\n",
       " array([29, 43, 31, 71, 54,  9, 58, 61]),\n",
       " array([30, 45,  2, 74,  2, 76, 67, 58]),\n",
       " array([25, 40, 73, 73, 76, 61, 24, 71]),\n",
       " array([27, 40, 61, 61, 68, 54,  2, 58]),\n",
       " array([29, 39, 54,  2, 66, 73, 33,  2]),\n",
       " array([ 1, 43, 73, 62, 54,  2, 72, 67])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:cs] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the next character after each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 33,  2, 72, 67, 73,  2, 68])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def each_tensor(items):\n",
    "    return [tensor(item) for item in items] \n",
    "\n",
    "class RnnMultiChar(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_fac)\n",
    "        self.dense_in_lin = nn.Linear(n_fac, n_hidden)\n",
    "        self.dense_hidden_lin = nn.Linear(n_hidden, n_hidden)\n",
    "        self.dense_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init()\n",
    "        \n",
    "    def dense_in(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense_in_lin(x)\n",
    "        x = F.relu(x, True)\n",
    "        return x\n",
    "            \n",
    "    def dense_hidden(self, x):\n",
    "        x = self.dense_hidden_lin(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, *c):\n",
    "        c_in = self.embedding(c[0])\n",
    "        hidden = self.dense_in(c_in)\n",
    "        \n",
    "        for i in range(1,cs):\n",
    "            c_in = self.embedding(c[i]) # x => torch.Size([B, 1, n_fac])\n",
    "            c_dense = self.dense_in(c_in)\n",
    "            hidden = self.dense_hidden(hidden)\n",
    "            hidden.add_(c_dense)\n",
    "        \n",
    "        c_out = self.dense_out(hidden)\n",
    "        \n",
    "        return c_out\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.uniform(self.embedding.weight, a=-0.05, b=0.05)\n",
    "        torch.nn.init.xavier_uniform(self.dense_in_lin.weight)\n",
    "        torch.nn.init.constant(self.dense_in_lin.bias, val=0.0)\n",
    "        torch.nn.init.eye(self.dense_hidden_lin.weight)\n",
    "        torch.nn.init.constant(self.dense_hidden_lin.bias, val=0.0)\n",
    "        torch.nn.init.xavier_uniform(self.dense_out.weight)\n",
    "        torch.nn.init.constant(self.dense_out.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnMultiChar (\n",
       "  (embedding): Embedding(85, 42)\n",
       "  (dense_in_lin): Linear (42 -> 256)\n",
       "  (dense_hidden_lin): Linear (256 -> 256)\n",
       "  (dense_out): Linear (256 -> 85)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RnnMultiChar()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 1175 batches [00:16, 70.66 batches/s, loss=2.6259]                    \n",
      "Epoch 2/4: 1175 batches [00:17, 66.64 batches/s, loss=2.2948]                    \n",
      "Epoch 3/4: 1175 batches [00:17, 66.26 batches/s, loss=2.1843]                    \n",
      "Epoch 4/4: 1175 batches [00:17, 65.76 batches/s, loss=2.1106]                    \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(each_tensor(xs), tensor(y), nb_epoch=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for ther')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN with PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleRNN layer does not exist in PyTorch (yet?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 42, 8, 85)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden, n_fac, cs, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly exactly equivalent to the RNN we built ourselves in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RnnMultiCharPytorch(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNNCell(input_size=n_fac, hidden_size=n_hidden, nonlinearity='relu')\n",
    "        self.dense_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, *c):\n",
    "        batch_size = c[0].size(0)\n",
    "        hidden = Variable(torch.zeros(batch_size, n_hidden))\n",
    "        # F.relu(F.linear(input, w_ih, b_ih)\n",
    "        for ci in c:\n",
    "            c_in = self.embedding(ci)\n",
    "            c_in = c_in.view(c_in.size(0), -1) # torch.Size([64, 42])\n",
    "            hidden = self.rnn(c_in, hidden)\n",
    "        \n",
    "        c_out = self.dense_out(hidden)\n",
    "        return c_out\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.uniform(self.embedding.weight, a=-0.05, b=0.05)\n",
    "        torch.nn.init.xavier_uniform(self.rnn.weight_ih)\n",
    "        torch.nn.init.constant(self.rnn.bias_ih, val=0.0)\n",
    "        torch.nn.init.eye(self.rnn.weight_hh)\n",
    "        torch.nn.init.constant(self.rnn.bias_hh, val=0.0)\n",
    "        torch.nn.init.xavier_uniform(self.dense_out.weight)\n",
    "        torch.nn.init.constant(self.dense_out.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnMultiCharPytorch (\n",
       "  (embedding): Embedding(85, 42)\n",
       "  (rnn): RNNCell(42, 256, nonlinearity=relu)\n",
       "  (dense_out): Linear (256 -> 85)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RnnMultiCharPytorch()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 1175 batches [00:17, 68.95 batches/s, loss=2.7699]                    \n",
      "Epoch 2/4: 1175 batches [00:20, 54.34 batches/s, loss=2.2835]                    \n",
      "Epoch 3/4: 1175 batches [00:20, 57.30 batches/s, loss=2.0904]                    \n",
      "Epoch 4/4: 1175 batches [00:29, 39.95 batches/s, loss=1.9617]                    \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(each_tensor(xs), tensor(y), nb_epoch=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for ther')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a sequence model, we can leave our input unchanged - but we have to change our output to a sequence (of course!)\n",
    "\n",
    "Here, c_out_dat is identical to c_in_dat, but moved across 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs, cs)]\n",
    "#            for n in range(cs)]\n",
    "c_out_dat = [[idx[i+n] for i in range(1, len(idx)-cs, cs)]\n",
    "            for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (75111,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = [np.stack(c) for c in c_out_dat]\n",
    "len(ys), ys[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading down each column shows one set of inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (75111,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:cs] for n in range(cs)]\n",
    "len(xs), xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (75111,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ys[n][:cs] for n in range(cs)]\n",
    "len(ys), ys[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RnnMultiOutput(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_fac)\n",
    "        self.dense_in_lin = nn.Linear(n_fac, n_hidden)\n",
    "        self.dense_hidden_lin = nn.Linear(n_hidden, n_hidden)\n",
    "        self.dense_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init()\n",
    "        \n",
    "    def dense_in(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense_in_lin(x)\n",
    "        x = F.relu(x, True)\n",
    "        return x\n",
    "            \n",
    "    def dense_hidden(self, x):\n",
    "        x = self.dense_hidden_lin(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, *c):\n",
    "        c_in = self.embedding(c[0])\n",
    "        hidden = self.dense_in(c_in)\n",
    "        \n",
    "        out = [self.dense_out(hidden)]\n",
    "        \n",
    "        for i in range(1,cs):\n",
    "            c_in = self.embedding(c[i]) # x => torch.Size([B, 1, n_fac])\n",
    "            c_dense = self.dense_in(c_in)\n",
    "            hidden = self.dense_hidden(hidden)\n",
    "            hidden.add_(c_dense)\n",
    "            out.append(self.dense_out(hidden))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.uniform(self.embedding.weight, a=-0.05, b=0.05)\n",
    "        torch.nn.init.xavier_uniform(self.dense_in_lin.weight)\n",
    "        torch.nn.init.constant(self.dense_in_lin.bias, val=0.0)\n",
    "        torch.nn.init.eye(self.dense_hidden_lin.weight)\n",
    "        torch.nn.init.constant(self.dense_hidden_lin.bias, val=0.0)\n",
    "        torch.nn.init.xavier_uniform(self.dense_out.weight)\n",
    "        torch.nn.init.constant(self.dense_out.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnMultiOutput (\n",
       "  (embedding): Embedding(85, 42)\n",
       "  (dense_in_lin): Linear (42 -> 256)\n",
       "  (dense_hidden_lin): Linear (256 -> 256)\n",
       "  (dense_out): Linear (256 -> 85)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RnnMultiOutput()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "\n",
    "# Bug in torchsample?\n",
    "trainer._has_multiple_loss_fns = False\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 1175 batches [00:22, 52.20 batches/s, loss=19.6869]                    \n",
      "Epoch 2/4: 1175 batches [00:23, 49.30 batches/s, loss=17.6566]                    \n",
      "Epoch 3/4: 1175 batches [00:24, 47.88 batches/s, loss=17.1526]                    \n",
      "Epoch 4/4: 1175 batches [00:24, 48.41 batches/s, loss=16.8499]                    \n"
     ]
    }
   ],
   "source": [
    "# TODO print each loss separately\n",
    "trainer.fit(each_tensor(xs), each_tensor(ys), nb_epoch=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def char_argmax(p):\n",
    "    # print(p.size())\n",
    "    v, i = torch.max(p, 0) # i is the result Tensor with the index locations of the maximum values\n",
    "    i = torch.max(i.data) # find any index (they are all max)\n",
    "    return chars[i]\n",
    "\n",
    "def get_nexts_multiple(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [tensor([i]) for i in idxs]\n",
    "    ps = trainer.predict(arrs)\n",
    "    print(list(inp))\n",
    "    return [char_argmax(p[0]) for p in ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 't', ' ', 'c', 'n', ' ']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_multiple(' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'o', 'r', 't', 'i', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_multiple(' part of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence model with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 42, 8, 85)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden, n_fac, cs, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our previous PyTorch model into a sequence model, simply return multiple outputs instead of a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RnnCellMultiOutput(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNNCell(input_size=n_fac, hidden_size=n_hidden, nonlinearity='relu')\n",
    "        self.dense_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init()\n",
    "\n",
    "    def forward(self, *c):\n",
    "        batch_size = c[0].size(0)\n",
    "        hidden = Variable(torch.zeros(batch_size, n_hidden))\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        for ci in c:\n",
    "            c_in = self.embedding(ci)\n",
    "            c_in = c_in.view(c_in.size(0), -1)\n",
    "            hidden = self.rnn(c_in, hidden)\n",
    "            out.append(self.dense_out(hidden))\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def init(self):\n",
    "        torch.nn.init.uniform(self.embedding.weight, a=-0.05, b=0.05)\n",
    "        torch.nn.init.xavier_uniform(self.rnn.weight_ih)\n",
    "        torch.nn.init.constant(self.rnn.bias_ih, val=0.0)\n",
    "        torch.nn.init.eye(self.rnn.weight_hh)\n",
    "        torch.nn.init.constant(self.rnn.bias_hh, val=0.0)\n",
    "        torch.nn.init.xavier_uniform(self.dense_out.weight)\n",
    "        torch.nn.init.constant(self.dense_out.bias, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnCellMultiOutput (\n",
       "  (embedding): Embedding(85, 42)\n",
       "  (rnn): RNNCell(42, 256, nonlinearity=relu)\n",
       "  (dense_out): Linear (256 -> 85)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RnnCellMultiOutput()\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "trainer = ModuleTrainer(model)\n",
    "trainer.set_optimizer(optim.Adam, lr=1e-3)\n",
    "trainer.set_loss(criterion)\n",
    "\n",
    "# Bug in torchsample?\n",
    "trainer._has_multiple_loss_fns = False\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 1175 batches [00:22, 53.17 batches/s, loss=19.3179]                    \n",
      "Epoch 2/4: 1175 batches [00:21, 53.67 batches/s, loss=15.9803]                    \n",
      "Epoch 3/4: 1175 batches [00:22, 51.81 batches/s, loss=15.0784]                    \n",
      "Epoch 4/4: 1175 batches [00:22, 53.04 batches/s, loss=14.6023]                    \n"
     ]
    }
   ],
   "source": [
    "# TODO print each loss separately\n",
    "trainer.fit(each_tensor(xs), each_tensor(ys), nb_epoch=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 's', ' ', 'c', 'n', ' ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_multiple(' this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful model with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
